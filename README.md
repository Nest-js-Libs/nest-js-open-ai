# M贸dulo OpenAI para NestJS

### @nest-js/open-ai

[![npm version](https://img.shields.io/npm/v/@nest-js/open-ai.svg)](https://www.npmjs.com/package/@nest-js/open-ai)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Este m贸dulo proporciona una integraci贸n completa con la API de OpenAI para aplicaciones NestJS, permitiendo utilizar f谩cilmente ChatGPT y otros modelos de OpenAI en tus proyectos.

##  Caracter铆sticas

- **Integraci贸n completa** con la API de OpenAI
- **Soporte para m煤ltiples modelos** (GPT-3.5-turbo, GPT-4, etc.)
- **Gesti贸n avanzada de prompts y conversaciones**
- **Configuraci贸n flexible** mediante opciones est谩ticas o din谩micas
- **Manejo inteligente de errores y reintentos autom谩ticos**
- **Limitaci贸n de tasa** (rate limiting) para optimizar costos
- **Soporte para prompts de sistema** para personalizar el comportamiento del asistente
- **Continuaci贸n de conversaciones** para mantener el contexto
- **Consulta de modelos disponibles** en tu cuenta de OpenAI

## 锔 Configuraci贸n

### Variables de Entorno

Agrega las siguientes variables de entorno a tu archivo `.env`:

```env
OPENAI_API_KEY=tu_api_key_de_openai
OPENAI_ORGANIZATION=tu_id_de_organizacion_opcional
OPENAI_DEFAULT_MODEL=gpt-3.5-turbo
OPENAI_MAX_TOKENS=1000
OPENAI_TEMPERATURE=0.7
OPENAI_TOP_P=1
OPENAI_FREQUENCY_PENALTY=0
OPENAI_PRESENCE_PENALTY=0
OPENAI_TIMEOUT=30000
OPENAI_MAX_RETRIES=3
```

### Configuraci贸n Est谩tica

Importa el m贸dulo en tu `app.module.ts` con configuraci贸n est谩tica:

```typescript
import { Module } from '@nestjs/common';
import { OpenAiModule } from '@nest-js/open-ai';

@Module({
  imports: [
    // otros m贸dulos...
    OpenAiModule.forRoot({
      apiKey: process.env.OPENAI_API_KEY,
      organization: process.env.OPENAI_ORGANIZATION,
      defaultModel: process.env.OPENAI_DEFAULT_MODEL || 'gpt-3.5-turbo',
      defaultOptions: {
        temperature: parseFloat(process.env.OPENAI_TEMPERATURE || '0.7'),
        maxTokens: parseInt(process.env.OPENAI_MAX_TOKENS || '1000'),
        topP: parseFloat(process.env.OPENAI_TOP_P || '1'),
        frequencyPenalty: parseFloat(process.env.OPENAI_FREQUENCY_PENALTY || '0'),
        presencePenalty: parseFloat(process.env.OPENAI_PRESENCE_PENALTY || '0'),
      },
      timeout: parseInt(process.env.OPENAI_TIMEOUT || '30000'),
      maxRetries: parseInt(process.env.OPENAI_MAX_RETRIES || '3'),
    }),
  ],
})
export class AppModule {}
```

### Configuraci贸n As铆ncrona

Alternativamente, puedes usar configuraci贸n as铆ncrona con `ConfigModule`:

```typescript
import { Module } from '@nestjs/common';
import { ConfigModule, ConfigService } from '@nestjs/config';
import { OpenAiModule } from '@nest-js/open-ai';

@Module({
  imports: [
    ConfigModule.forRoot({
      // Configuraci贸n del m贸dulo de configuraci贸n
    }),
    OpenAiModule.forRootAsync({
      imports: [ConfigModule],
      useFactory: async (configService: ConfigService) => ({
        apiKey: configService.get<string>('OPENAI_API_KEY'),
        organization: configService.get<string>('OPENAI_ORGANIZATION'),
        defaultModel: configService.get<string>('OPENAI_DEFAULT_MODEL') || 'gpt-3.5-turbo',
        defaultOptions: {
          temperature: parseFloat(configService.get('OPENAI_TEMPERATURE') || '0.7'),
          maxTokens: parseInt(configService.get('OPENAI_MAX_TOKENS') || '1000'),
          topP: parseFloat(configService.get('OPENAI_TOP_P') || '1'),
          frequencyPenalty: parseFloat(configService.get('OPENAI_FREQUENCY_PENALTY') || '0'),
          presencePenalty: parseFloat(configService.get('OPENAI_PRESENCE_PENALTY') || '0'),
        },
        timeout: parseInt(configService.get('OPENAI_TIMEOUT') || '30000'),
        maxRetries: parseInt(configService.get('OPENAI_MAX_RETRIES') || '3'),
      }),
      inject: [ConfigService],
    }),
  ],
})
export class AppModule {}
```

##  Uso B谩sico

### Inyecci贸n del Servicio

```typescript
import { Injectable } from '@nestjs/common';
import { OpenAiService } from '@nest-js/open-ai';

@Injectable()
export class TuServicio {
  constructor(private readonly openAiService: OpenAiService) {}

  // M茅todos de tu servicio...
}
```

### Generaci贸n de Texto Simple

```typescript
async generarTexto(prompt: string): Promise<string> {
  return await this.openAiService.generateText(prompt, {
    model: 'gpt-3.5-turbo',
    temperature: 0.7,
    maxTokens: 150,
  });
}
```

### Completaci贸n de Chat

```typescript
async chatear(mensajes: Array<{ role: string; content: string }>): Promise<string> {
  return await this.openAiService.createChatCompletion({
    messages: mensajes,
    temperature: 0.5,
    maxTokens: 500,
  });
}
```

### Uso de Prompt de Sistema

```typescript
async responderConContexto(pregunta: string): Promise<string> {
  return await this.openAiService.generateWithSystemPrompt({
    systemPrompt: 'Eres un asistente experto en programaci贸n NestJS',
    userPrompt: pregunta,
    options: {
      temperature: 0.3,
      maxTokens: 800,
    },
  });
}
```

### Continuaci贸n de Conversaciones

```typescript
async continuarConversacion(
  conversacionPrevia: Array<{ role: string; content: string }>,
  nuevoMensaje: string
): Promise<string> {
  return await this.openAiService.continueConversation({
    conversation: conversacionPrevia,
    newMessage: nuevoMensaje,
    options: {
      temperature: 0.7,
      maxTokens: 500,
    },
  });
}
```

### Consulta de Modelos Disponibles

```typescript
async obtenerModelos(): Promise<string[]> {
  return await this.openAiService.getAvailableModels();
}
```

##  Ejemplos Avanzados

### Implementaci贸n de un Controlador

```typescript
import { Body, Controller, Post } from '@nestjs/common';
import { OpenAiService } from '@nest-js/open-ai';
import { ChatRole } from '@nest-js/open-ai';

@Controller('ia')
export class IaController {
  constructor(private readonly openAiService: OpenAiService) {}

  @Post('asistente')
  async obtenerRespuesta(@Body() body: { pregunta: string }) {
    const respuesta = await this.openAiService.generateWithSystemPrompt({
      systemPrompt: 'Eres un asistente virtual amigable y servicial que ayuda a los usuarios con sus consultas.',
      userPrompt: body.pregunta,
      options: {
        temperature: 0.7,
        maxTokens: 500,
      },
    });
    
    return { respuesta };
  }

  @Post('chat')
  async chatear(@Body() body: { mensajes: Array<{ role: string; content: string }> }) {
    const respuesta = await this.openAiService.createChatCompletion({
      messages: body.mensajes,
      temperature: 0.5,
    });
    
    return { respuesta };
  }
}
```

### Personalizaci贸n de Par谩metros

Puedes ajustar los siguientes par谩metros para controlar la generaci贸n de texto:

- **temperature**: Controla la aleatoriedad (0-1). Valores m谩s bajos = respuestas m谩s deterministas.
- **maxTokens**: Limita la longitud de la respuesta.
- **topP**: Controla la diversidad mediante muestreo de n煤cleo.
- **frequencyPenalty**: Penaliza palabras repetidas (0-2).
- **presencePenalty**: Penaliza tokens ya utilizados (0-2).

## З Integraci贸n con Otros Servicios

### Ejemplo con Servicio de Traducci贸n

```typescript
@Injectable()
export class TraductorService {
  constructor(private readonly openAiService: OpenAiService) {}

  async traducir(texto: string, idiomaDestino: string): Promise<string> {
    return await this.openAiService.generateWithSystemPrompt({
      systemPrompt: `Eres un traductor profesional. Traduce el siguiente texto al idioma ${idiomaDestino} manteniendo el tono y contexto original.`,
      userPrompt: texto,
      options: {
        temperature: 0.3,
        maxTokens: 1000,
      },
    });
  }
}
```

##  Limitaciones

- **Costos**: Gestiona adecuadamente los costos asociados con las llamadas a la API de OpenAI.
- **L铆mites de tokens**: Cada modelo tiene un l铆mite m谩ximo de tokens por solicitud (contexto + respuesta).
- **Rate limiting**: OpenAI impone l铆mites de tasa que pueden afectar aplicaciones con alto volumen.
- **Latencia**: Las respuestas pueden tardar varios segundos, especialmente con modelos m谩s grandes.

##  Contribuciones

Las contribuciones son bienvenidas. Por favor, abre un issue o pull request en el repositorio.

##  Licencia

Este proyecto est谩 licenciado bajo la [Licencia MIT](https://opensource.org/licenses/MIT).